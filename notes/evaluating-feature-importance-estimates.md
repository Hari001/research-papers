# Evaluating feature importance estimates

Authors: Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim
Arxiv Link: https://arxiv.org/abs/1806.10758

## Importance of feature estimation

Identifying the accuracy of a feature important estimate is very useful for interpreting and evaluating a model.

## Challenges
1. Understanding or interpreting what a model represents is difficult when the dimensionality of the input space is high.
2. There is not ground truth for feature importance

## Proposed Benchmark for Evaluation

**ROAR** - **R**em**O**ve **A**nd **R**etrain

### Process

1. Compute the feature importance of all input feature for all estimators (to be evaluated) for a given model.
2. Remove a fraction of the most importnt features from each data point (Removing features => Replacing them with an uninformative value e.g., the mean of the feature).
3. Retrain the model with the modified inputs.
4. Measure the degradation of performance.

The estimator with the most performance degradation is the best one.

## Experimental Details

+ ResNet-50 model
+ Imagenet Dataset (1.28 million training images + 50k validation images)

## Evaluated estimators

+ Gradients or sensitivity heatmap
+ Guided backprop
+ Integrated gradients

Also, based on previous research by Smilkov et. al. 2017, for each of these basic estimators, we square and average noisy estimates (generated by injecting inputs with Gaussian noise)

## Baseline Estimator

Assigning random importance to the input features


## Key Results

1. The basic estimators (without squaring and averaging) often perform worse than the baseline estimator.
2. Sqauring and averaging based estimators outperform all the other ones
3. Only averaging does not produce performance gains.
4. Only squaring gives a small performance gain

